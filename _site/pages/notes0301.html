
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Discussion 5 - 03/01</title>
    <meta name="description" content="notes, links, example code and exercises">
    <meta name="author" content="Peigen Zhou">

    <!-- Enable responsive viewport -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="/assets/themes/twitter/bootstrap/css/bootstrap.2.2.2.min.css" rel="stylesheet">
    <link href="/assets/themes/twitter/css/style.css?body=1" rel="stylesheet" type="text/css" media="all">
    <link href="/assets/themes/twitter/css/main.css" rel="stylesheet" type="text/css" media="all">

    <!-- Le fav and touch icons -->

    <!-- atom & rss feed -->
    <link href="nil" type="application/atom+xml" rel="alternate" title="Sitewide ATOM Feed">
    <link href="nil" type="application/rss+xml" rel="alternate" title="Sitewide RSS Feed">

  </head>

  <body>
    <div class="navbar">
      <div class="navbar-inner">
        <div class="container-narrow">
          <a class="brand" href="/">Data Science Practicum - Stat 628 - Discussion</a>
          <ul class="nav">
              <li><a href="/pages/schedule.html">Schedule</a></li>
              <li><a href="/pages/topics.html">Topics</a></li>
          </ul>

        </div>
      </div>
    </div>

    <div class="container-narrow">

      <div class="content">
        
<div class="page-header">
  <h2>Discussion 5 - 03/01 </h2>
</div>

<div class="row-fluid">
  <div class="span12">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async=""></script>

<h3 id="split">split</h3>
<hr />

<p>In the unix shell, you can type <code class="highlighter-rouge">man split</code> to see the manual of split function.</p>

<p>Yelp data, the json file is large for a personal laptop. One option, store files sperately on the disk, and process each file once a time.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>man split
split <span class="nt">-l</span> 100000 review_train.json
</code></pre></div></div>

<p>It will split review_train.json into small pieces, make your life much easier.</p>

<h3 id="gradient-descent">Gradient Descent</h3>
<hr />

<ul>
  <li><a href="../assets/python/SGD-discussion.ipynb">jupyter notebook</a></li>
</ul>

<p><a href="https://en.wikipedia.org/wiki/Gradient_descent">Wiki: gradient descent</a></p>

<p>In general, our optimization problem can be viewd as following: we are minimizing the <strong>Objective</strong> function with respect to <script type="math/tex">\theta</script>:</p>

<script type="math/tex; mode=display">\text{argmin}_{\theta}\ L(\theta , x) \quad \textbf{Objective}\ \text{function},</script>

<p>for simplicity, we only consider the class of convex function. (Only one minimal point)</p>

<p><strong>Goal</strong></p>

<p>Generate a sequence of <script type="math/tex">\theta_{i}</script>, such that</p>

<script type="math/tex; mode=display">L(\theta_{1}, x) \ge L(\theta_{2}, x) \ge \dots \ge L(\theta_i, x) \ge L(\theta_{i+1}, x) \ge \dots</script>

<p><strong>Steps</strong></p>

<ol>
  <li>Calculate the gradient of objective function <script type="math/tex">\nabla L(\theta_i)</script></li>
  <li>Move to the negative direction of derivative <script type="math/tex">-\nabla L(\theta_i)</script>.</li>
  <li>Update <script type="math/tex">\theta_{i + 1}=\theta_{i}-\gamma\nabla L(\theta_{i}, x) \gamma > 0</script></li>
  <li>Check whether converge. Does <script type="math/tex">\nabla L(\theta_i)</script> close to zero? Since if it is a minimal point, <script type="math/tex">\nabla L(\theta) = 0</script></li>
</ol>

<h4 id="examples">Examples</h4>

<p>The gradient descent algorithm is applied to find a local minimum of the function <script type="math/tex">f(x)=x^{4}-3x^{3}+2</script>, with derivative <script type="math/tex">f'{(x)}=4x^{3}-9x^{2}</script>. Here is an implementation is:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># from wiki: https://en.wikipedia.org/wiki/Gradient_descent</span>
<span class="c"># From calculation, it is expected that the local minimum occurs at x=9/4</span>

<span class="n">cur_x</span> <span class="o">=</span> <span class="mi">3</span> <span class="c"># The algorithm starts at x=6</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="c"># step size multiplier</span>
<span class="n">precision</span> <span class="o">=</span> <span class="mf">0.00001</span>
<span class="n">previous_step_size</span> <span class="o">=</span> <span class="mi">1</span> 
<span class="n">max_iters</span> <span class="o">=</span> <span class="mi">10000</span> <span class="c"># maximum number of iterations</span>
<span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span> <span class="c">#iteration counter</span>

<span class="n">ff</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">df</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">9</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>

<span class="n">xpath</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">ypath</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">while</span> <span class="n">previous_step_size</span> <span class="o">&gt;</span> <span class="n">precision</span> <span class="ow">and</span> <span class="n">iters</span> <span class="o">&lt;</span> <span class="n">max_iters</span><span class="p">:</span>
    <span class="n">xpath</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_x</span><span class="p">)</span>
    <span class="n">ypath</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ff</span><span class="p">(</span><span class="n">cur_x</span><span class="p">))</span>
    <span class="n">prev_x</span> <span class="o">=</span> <span class="n">cur_x</span>
    <span class="n">cur_x</span> <span class="o">-=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">df</span><span class="p">(</span><span class="n">prev_x</span><span class="p">)</span>
    <span class="n">previous_step_size</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">cur_x</span> <span class="o">-</span> <span class="n">prev_x</span><span class="p">)</span>
    <span class="n">iters</span><span class="o">+=</span><span class="mi">1</span>

<span class="k">print</span><span class="p">(</span><span class="s">"The local minimum occurs at"</span><span class="p">,</span> <span class="n">cur_x</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="linear-regression">Linear regression</h4>

<p>Letâ€™s first start with a simple example, the coefficient estimation in linear regression (OLS)</p>

<script type="math/tex; mode=display">Y = X\beta + \epsilon, \epsilon \sim N(0, \sigma^2 I) \quad \text{// Statistical Model}</script>

<script type="math/tex; mode=display">\text{argmin}_{\theta} L(\beta, X) = \frac{1}{2N}(Y - X\beta)^T(Y - X\beta) \quad \text{// Negative log-likelihood}</script>

<script type="math/tex; mode=display">= \frac{1}{2N} (Y^TY - 2Y^TX\beta + \beta^T X^T X \beta)</script>

<p><script type="math/tex">N</script> is the sample size</p>

<ul>
  <li>OLS exact solution: <script type="math/tex">\hat{\beta} = (X^TX)^{-1}X^TY</script></li>
  <li>Gradient descent:</li>
</ul>

<script type="math/tex; mode=display">\nabla L(\beta_i , X) = \frac{-X^T(Y - X \beta_i)}{N} = \frac{-X^T Y + X^TX\beta_i}{N}\quad \text{// Gradient of } \beta_i</script>

<script type="math/tex; mode=display">\theta_{i + 1} = \beta_i -\gamma\nabla L(\beta_i, x) \quad \text{// Update step}</script>

<script type="math/tex; mode=display">\text{Check }\nabla L(\beta_i, X)</script>

<h5 id="data">Data</h5>

<p>We use a simple example to perform the task. Here we set <code class="highlighter-rouge">N = 100000</code> and <code class="highlighter-rouge">p = 5</code>.</p>

<script type="math/tex; mode=display">\beta = \begin{bmatrix} 1\\ 2\\ 3\\ -2\\ -3\end{bmatrix} \quad \text{True}</script>

<script type="math/tex; mode=display">X \sim N(0, I)</script>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">beta_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="err">@</span> <span class="n">beta_true</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">lm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">X</span><span class="p">)</span> <span class="err">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">y</span>

<span class="n">lm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<h5 id="gd-for-linear-regression">GD for linear regression</h5>

<p>Letâ€™s first define function <code class="highlighter-rouge">loss</code>, <code class="highlighter-rouge">derivative</code>.</p>

<p><strong>Please</strong> note here, these implementations are not smart, some of the computations are duplicated. 
You can easily make it much faster by moving some lines around. :)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">xtx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">x</span> <span class="c"># c</span>
    <span class="n">xty</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">y</span> <span class="c"># c</span>
    <span class="n">yty</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">y</span> <span class="c"># c</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">yty</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">beta</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">xty</span> <span class="o">+</span> <span class="n">beta</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">xtx</span> <span class="err">@</span> <span class="n">beta</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">N</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">xtx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">x</span> <span class="c"># c</span>
    <span class="n">xty</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">y</span> <span class="c"># c</span>
    <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="n">xty</span> <span class="o">+</span> <span class="n">xtx</span> <span class="err">@</span> <span class="n">beta</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gdescent_lm</span><span class="p">(</span><span class="n">init</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">derivative</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span>
                <span class="n">precision</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
                <span class="n">previous_step_size</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">max_iters</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">):</span>
    
    <span class="n">cur_x</span> <span class="o">=</span> <span class="n">init</span> <span class="c"># Initial points</span>
    <span class="n">loss_path</span> <span class="o">=</span> <span class="p">[</span><span class="n">loss</span><span class="p">(</span><span class="n">cur_x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)]</span>
    
    <span class="n">iters</span> <span class="o">=</span> <span class="mi">1</span> <span class="c">#iteration counter</span>
    
    <span class="k">while</span> <span class="n">previous_step_size</span> <span class="o">&gt;</span> <span class="n">precision</span> <span class="ow">and</span> <span class="n">iters</span> <span class="o">&lt;</span> <span class="n">max_iters</span><span class="p">:</span>
        
        <span class="n">delta</span> <span class="o">=</span> <span class="c">#write yourself, step</span>
        <span class="n">cur_x</span> <span class="o">=</span> <span class="c">#update as new step</span>
        
        <span class="n">previous_step_size</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c"># check step size</span>
        
        <span class="n">cur_y</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">cur_x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">cur_y</span> <span class="o">&gt;</span> <span class="n">loss_path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Current stepsize {} maybe too large, change to {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">*</span> <span class="mf">0.9</span><span class="p">))</span>
            <span class="n">gamma</span> <span class="o">*=</span> <span class="mf">0.9</span>
        
        <span class="n">iters</span> <span class="o">+=</span> <span class="mi">1</span>
        
        <span class="k">if</span> <span class="n">iters</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Current iteration: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">iters</span><span class="p">))</span>
        
        <span class="n">loss_path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_y</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">cur_x</span><span class="p">,</span> <span class="n">loss_path</span><span class="p">,</span> <span class="n">iters</span>
</code></pre></div></div>

<h4 id="stochastic-gradient-descent">Stochastic Gradient Descent</h4>

<p><code class="highlighter-rouge">@</code> is a very expensive step, also <code class="highlighter-rouge">.T</code> in the gradient evaluation. Because it involves summation of <strong>all the data points</strong>.</p>

<p>Stochastic gradient descent only evaluate one data point at once or a batch (<script type="math/tex">\ll N</script>) of data points at once, so that it will lower the cost of summation overall all the data.</p>

<h5 id="steps">Steps:</h5>

<ol>
  <li>Set batch i = some numbers. The batch means a chunk of (x, y) where the batch size usually <script type="math/tex">\ll N</script></li>
  <li>Calculate the gradient <script type="math/tex">\nabla L(\theta_i, X_{\text{batch i}})</script></li>
  <li>Move the negative step of the <script type="math/tex">-\gamma\nabla L(\theta_i, X_{\text{batch i}})</script></li>
  <li>Update <script type="math/tex">\theta_{i + 1} = \theta_i -\gamma\nabla L(\theta_i, X_{\text{batch i}})</script></li>
  <li>Check convergence</li>
  <li>get another batch, go back to step 1</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sgd_lm</span><span class="p">(</span><span class="n">init</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">derivative</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> 
           <span class="n">batchSize</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
           <span class="n">precision</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
           <span class="n">previous_step_size</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">N</span> <span class="o">&lt;</span> <span class="n">batchSize</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Current sample size: {}, batch size: {}</span><span class="se">\n</span><span class="s">Update batch size to 10</span><span class="si">% </span><span class="s">samples: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">N</span> <span class="o">//</span> <span class="mi">10</span><span class="p">))</span>
        <span class="n">batchSize</span> <span class="o">=</span> <span class="n">N</span> <span class="o">//</span> <span class="mi">10</span>
    
    <span class="n">cur_x</span> <span class="o">=</span> <span class="n">init</span>
    <span class="n">e</span> <span class="o">=</span> <span class="mi">0</span> <span class="c">## Start from first epoch</span>
    <span class="n">indexN</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">loss_path</span><span class="p">)</span>
    <span class="k">while</span> <span class="n">e</span> <span class="o">&lt;</span> <span class="n">epoch</span> <span class="ow">and</span> <span class="n">previous_step_size</span> <span class="o">&gt;</span> <span class="n">precision</span><span class="p">:</span>
        
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indexN</span><span class="p">)</span> <span class="c">## give a random sequence of index</span>
        <span class="n">parts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">indexN</span><span class="p">,</span> <span class="n">N</span> <span class="o">//</span> <span class="n">batchSize</span><span class="p">)</span>
        <span class="n">cur_y</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">preloss</span> <span class="o">=</span> <span class="n">loss_path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> 
        <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">parts</span><span class="p">:</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="c">#write yourself</span>
            <span class="n">cur_x</span> <span class="o">=</span> <span class="c">#write yourself</span>
            <span class="n">curloss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">cur_x</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">index</span><span class="p">,],</span> <span class="n">y</span><span class="p">[</span><span class="n">index</span><span class="p">])</span> 
            <span class="n">cur_y</span> <span class="o">+=</span> <span class="n">curloss</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
            
            
            <span class="k">if</span> <span class="n">curloss</span> <span class="o">&gt;</span> <span class="n">preloss</span><span class="p">:</span>
                <span class="c"># print("Current stepsize {} maybe too large, change to {}".format(gamma, gamma * 0.99))</span>
                <span class="n">gamma</span> <span class="o">*=</span> <span class="mf">0.99</span>
            <span class="n">loss_path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">curloss</span><span class="p">)</span>
        <span class="c">#loss_path.append(cur_y / N)</span>
        
        <span class="n">previous_step_size</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Finished epoch: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
                
        <span class="n">e</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">cur_x</span><span class="p">,</span> <span class="n">loss_path</span><span class="p">,</span> <span class="n">e</span>
</code></pre></div></div>

<h3 id="practice">Practice</h3>

<p>How to perfom this in ridge regression?</p>

<p>Other optimizer: <a href="http://stanford.edu/~boyd/admm.html">Admm</a>, <a href="http://cs231n.github.io/neural-networks-3/#sgd">CS231n-parameters update</a></p>

<hr />


  </div>
</div>


      </div>
      <hr>
      <footer>
        <p><small>
          <!-- start of footer -->
          &copy Spring 2019 &nbsp;-&nbsp;
          <a href="http://peigenzhou.com">Peigen Zhou</a> &nbsp;
          Themes based on  <a href="http://cecileane.github.io/computingtools/">Stat 679</a> &nbsp;and <a href="http://kbroman.org/">Karl Broman</a>
          <!-- end of footer -->
        </small></p>
      </footer>

    </div>

    
  </body>
</html>

