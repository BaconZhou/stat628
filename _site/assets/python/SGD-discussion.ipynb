{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "[Wiki: gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)\n",
    "\n",
    "In general, our optimization problem can be viewd as following: we are minimizing the **Objective** function with respect to $\\theta$:\n",
    "\n",
    "$$\\text{argmin}_{\\theta}\\ L(\\theta | x) \\quad \\textbf{Objective}\\ \\text{function},$$\n",
    "\n",
    "for simplicity, we only consider the class of convex function. (Only one minimal point)\n",
    "\n",
    "**Goal**\n",
    "\n",
    "Generate a sequence of $\\theta_i$ such that $L(\\theta_1|x) \\ge L(\\theta_2|x) \\ge \\dots \\ge L(\\theta_i|x) \\ge L(\\theta_{i+1}|x) \\ge \\dots$\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Calculate the gradient of objective function$\\nabla L(\\theta_i)$\n",
    "2. Move to the negative direction of derivative $-\\nabla L(\\theta_i)$\n",
    "3. Update $\\theta_{i + 1} = \\theta_i -\\gamma\\nabla L(\\theta_i | x), \\ \\gamma > 0$\n",
    "4. Check whether converge. Does $\\nabla L(\\theta_i)$ close to zero? Since if it is a minimal point, $\\nabla L(\\theta) = 0$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "The gradient descent algorithm is applied to find a local minimum of the function $f(x)=x^{4}-3x^{3}+2$, with derivative $f'{(x)}=4x^{3}-9x^{2}$. Here is an implementation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From calculation, it is expected that the local minimum occurs at x=9/4\n",
    "\n",
    "cur_x = 3 # The algorithm starts at x=6\n",
    "gamma = 0.01 # step size multiplier\n",
    "precision = 0.00001\n",
    "previous_step_size = 1 \n",
    "max_iters = 10000 # maximum number of iterations\n",
    "iters = 0 #iteration counter\n",
    "\n",
    "ff = lambda x: x**4 - 3*x**3 + 2\n",
    "df = lambda x: 4 * x**3 - 9 * x**2\n",
    "\n",
    "xpath = []\n",
    "ypath = []\n",
    "\n",
    "while previous_step_size > precision and iters < max_iters:\n",
    "    xpath.append(cur_x)\n",
    "    ypath.append(ff(cur_x))\n",
    "    prev_x = cur_x\n",
    "    cur_x -= gamma * df(prev_x)\n",
    "    previous_step_size = abs(cur_x - prev_x)\n",
    "    iters+=1\n",
    "\n",
    "print(\"The local minimum occurs at\", cur_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_path(x, y):\n",
    "    \n",
    "    xm, xM = min(x) - 0.5, max(x) + 1\n",
    "    ym, yM = min(y) - 0.5, max(y) + 1\n",
    "\n",
    "    data = [dict(x = x, y = y, mode = 'lines', line = dict(width = 2, color = 'blue')),\n",
    "           dict(x = x, y = y, mode = 'lines', line = dict(width = 2, color = 'blue'))]\n",
    "    layout = dict(xaxis = dict(range = [xm, xM], autorange = False, zeroline = False, title = \"X\"), \n",
    "                  yaxis = dict(range = [ym, yM], autorange = False, zeroline = False, title = \"Loss\"),\n",
    "                  title = \"Plot of function curve\",\n",
    "                  hovermode = 'closest',\n",
    "                  updatemenus= [{'type': 'buttons',\n",
    "                           'buttons': [{'label': 'Play',\n",
    "                                        'method': 'animate',\n",
    "                                        'args': [None]}]}])\n",
    "    N = len(x)\n",
    "    mod = 1\n",
    "    if N > 20:\n",
    "        mod = int(N / 20)\n",
    "    \n",
    "    frames = [dict(data = [dict(x = [x[k]], y = [y[k]], \n",
    "                   mode = 'markers',\n",
    "                   marker = dict(color='red', size=10))]) for k in range(N) if k % mod == 0]\n",
    "    figure1 = dict(data = data, layout = layout, frames = frames)\n",
    "    return iplot(figure1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path(xpath, ypath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Linear regression OLS\n",
    "\n",
    "Let's first start with a simple example, the coefficient estimation in Linear regression (OLS)\n",
    "\n",
    "\\begin{align}\n",
    "    Y &= X\\beta + \\epsilon, \\ \\epsilon \\sim N(0, \\sigma^2 I) \\quad \\text{// Statistical Model} \\\\\n",
    "    \\text{argmin}_{\\theta}\\ L(\\beta | X) &= \\frac{1}{2N}(Y - X\\beta)^T(Y - X\\beta) \\quad \\text{// Negative log-likelihood} \\\\\n",
    "    &= \\frac{1}{2N} (Y^TY - 2Y^TX\\beta + \\beta^T X^T X \\beta)\\\\\n",
    "\\end{align}\n",
    "\n",
    "$N$ is the sample size\n",
    "\n",
    "- OLS result: $\\hat{\\beta}\\ = (X^TX)^{-1}X^TY$\n",
    "- Gradient descent:\n",
    "\\begin{align}\n",
    "    &\\nabla L(\\beta_i | X) = \\frac{-X^T(Y - X \\beta_i)}{N} = \\frac{-X^T Y + X^TX\\beta_i}{N}\\quad \\text{// Gradient of } \\beta_i \\\\\n",
    "    &\\theta_{i + 1} = \\beta_i -\\gamma\\nabla L(\\beta_i | x) \\quad \\text{// Update step}\\\\\n",
    "    &\\text{Check }\\nabla L(\\beta_i | X)\n",
    "\\end{align}\n",
    "\n",
    "### Data\n",
    "\n",
    "We use a simple example to perform the task. Here we set `N = 100000` and `p = 5`. \n",
    "\n",
    "$$\\beta = \\begin{bmatrix} 1\\\\ 2\\\\ 3\\\\ -2\\\\ -3\\end{bmatrix} \\quad \\text{True}$$\n",
    "$$X \\sim N(0, I)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000\n",
    "p = 5\n",
    "beta_true = np.array([1., 2.0, 3.0, -2.0, -3.0])\n",
    "x = np.random.randn(N, p)\n",
    "y = x @ beta_true + np.random.randn(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm(X, y):\n",
    "    return np.linalg.inv(X.T @ X) @ X.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "lm(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(beta, x, y):\n",
    "    N = len(y)\n",
    "    xtx = x.T @ x\n",
    "    xty = x.T @ y\n",
    "    yty = y.T @ y\n",
    "    return (yty - 2 * beta.T @ xty + beta.T @ xtx @ beta) / (2 * N)\n",
    "\n",
    "def derivative(beta, x, y):\n",
    "    N = len(y)\n",
    "    xtx = x.T @ x\n",
    "    xty = x.T @ y\n",
    "    return (-xty + xtx @ beta) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdescent_lm(init, x, y, derivative, loss,\n",
    "                precision = 0.001, gamma = 0.001,\n",
    "                previous_step_size = 1, max_iters = 10000):\n",
    "    \n",
    "    cur_x = init\n",
    "    loss_path = [loss(cur_x, x, y)]\n",
    "    \n",
    "    iters = 1 #iteration counter\n",
    "    while previous_step_size > precision and iters < max_iters:\n",
    "        \n",
    "        delta = # write yourself\n",
    "        cur_x = # update step\n",
    "        \n",
    "        previous_step_size = np.linalg.norm(delta, ord=2)\n",
    "        \n",
    "        cur_y = loss(cur_x, x, y)\n",
    "        \n",
    "        if cur_y > loss_path[-1]:\n",
    "            print(\"Current stepsize {} maybe too large, change to {}\".format(gamma, gamma * 0.9))\n",
    "            gamma *= 0.9\n",
    "        \n",
    "        iters += 1\n",
    "        if iters % 50 == 0:\n",
    "            print(\"Current iteration: {}\".format(iters))\n",
    "        \n",
    "        loss_path.append(cur_y)\n",
    "\n",
    "    return cur_x, loss_path, iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time beta_gd, loss_path_gd, steps_gd = gdescent_lm(np.array([0., 0., 0., 0., 0.]), x, y, derivative, loss, gamma = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path([i for i in range(steps)], loss_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "`@` is a very expensive step, also `.T` in the gradient evaluation. Because it involves summation of all the data points. Stochastic Gradient Descent only evaluate one data point at once or a batch ($\\ll N$) of data points at once. \n",
    "\n",
    "## Steps:\n",
    "\n",
    "1. Set batch i = some numbers. The batch means a chunk of (x, y) where the batch size usually $\\ll N$\n",
    "2. Calculate the gradient $\\nabla L(\\theta_i | X_{\\text{batch i}})$\n",
    "3. Move the negative step of the $-\\gamma\\nabla L(\\theta_i | X_{\\text{batch i}})$\n",
    "4. Update $\\theta_{i + 1} = \\theta_i -\\gamma\\nabla L(\\theta_i | X_{\\text{batch i}})$\n",
    "5. Check convergence\n",
    "6. get another batch, go back to step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_lm(init, x, y, derivative, loss, \n",
    "           batchSize = 500, epoch = 10,\n",
    "           precision = 0.001, gamma = 0.001,\n",
    "           previous_step_size = 1):\n",
    "    N = len(y)\n",
    "    if N < batchSize:\n",
    "        print(\"Current sample size: {}, batch size: {}\\nUpdate batch size to 10% samples: {}\".format(N, batchSize, N // 10))\n",
    "        batchSize = N // 10\n",
    "    \n",
    "    cur_x = init\n",
    "    e = 0 ## Start from first epoch\n",
    "    indexN = np.arange(N)\n",
    "\n",
    "    while e < epoch and previous_step_size > precision:\n",
    "        \n",
    "        np.random.shuffle(indexN) ## give a random sequence of index\n",
    "        parts = np.array_split(indexN, N // batchSize)\n",
    "        cur_y = 0\n",
    "        \n",
    "        preloss = loss_path[-1] \n",
    "        \n",
    "        for index in parts:\n",
    "            \n",
    "            delta = # write yourself\n",
    "            cur_x = # update step\n",
    "            curloss = loss(cur_x, x[index,], y[index]) \n",
    "            cur_y += curloss * len(index)\n",
    "            \n",
    "            \n",
    "            if curloss > preloss:\n",
    "                # print(\"Current stepsize {} maybe too large, change to {}\".format(gamma, gamma * 0.99))\n",
    "                gamma *= 0.99\n",
    "            loss_path.append(curloss)\n",
    "        #loss_path.append(cur_y / N)\n",
    "        \n",
    "        previous_step_size = np.linalg.norm(delta, ord=2)\n",
    "        print(\"Finished epoch: {}\".format(e))\n",
    "                \n",
    "        e += 1\n",
    "    return cur_x, loss_path, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time beta_sgd, loss_path_sgd, steps_sgd = sgd_lm(np.array([0., 0., 0., 0., 0.]), x, y, derivative, loss, batchSize=100, gamma = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path([i for i in range(len(loss_path))], loss_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice\n",
    "\n",
    "Consider use gd and sgd for ridge regression? or neural network\n",
    "\n",
    "You basically only need to create both `derivative` and `loss` function for ridge. Also consider the tunning parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_derivative(beta, x, y, tunning = 1):\n",
    "    \"\"\"write you ridge derivative\"\"\"\n",
    "def ridge_loss(beta, x, y, tunning = 1):\n",
    "    \"\"\"write you ridge loss\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_sgd, loss_path_sgd, steps_sgd = sgd_lm(np.array([0., 0., 0., 0., 0.]), x, y, ridge_derivative, ridge_loss, batchSize=100, gamma = 0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
